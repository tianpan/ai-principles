---
title: è®­ç»ƒä¸æ¨ç†
description: Mini Transformer è®­ç»ƒå’Œæ¨ç†æµç¨‹
---

# è®­ç»ƒä¸æ¨ç†

æœ¬ç« ä»‹ç»å¦‚ä½•è®­ç»ƒ Mini Transformer å¹¶ä½¿ç”¨å®ƒè¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚

---

## 1. æ•°æ®å‡†å¤‡

æˆ‘ä»¬ä½¿ç”¨èå£«æ¯”äºšçš„è‘—åç‰‡æ®µä½œä¸º toy æ•°æ®é›†ï¼š

```python
# data.py
SHAKESPEARE_TEXT = """
To be, or not to be, that is the question:
Whether 'tis nobler in the mind to suffer
The slings and arrows of outrageous fortune,
Or to take arms against a sea of troubles
...
"""

class CharTokenizer:
    """å­—ç¬¦çº§åˆ†è¯å™¨"""
    def __init__(self, text: str):
        self.chars = sorted(list(set(text)))
        self.vocab_size = len(self.chars)
        self.char_to_idx = {c: i for i, c in enumerate(self.chars)}
        self.idx_to_char = {i: c for i, c in enumerate(self.chars)}

    def encode(self, text): return [self.char_to_idx[c] for c in text]
    def decode(self, ids): return ''.join([self.idx_to_char[i] for i in ids])
```

**éªŒè¯**ï¼š
```python
>>> tokenizer = CharTokenizer(SHAKESPEARE_TEXT)
>>> tokenizer.vocab_size
45  # è¯è¡¨å¤§å°ï¼ˆå”¯ä¸€å­—ç¬¦æ•°ï¼‰
>>> tokenizer.decode(tokenizer.encode("To be"))
'To be'  # âœ“ ç¼–ç è§£ç ä¸€è‡´
```

---

## 2. è®­ç»ƒé…ç½®

```python
config = {
    'vocab_size': 45,
    'd_model': 64,
    'num_heads': 4,
    'num_layers': 2,
    'd_ff': 256,
    'seq_len': 32,
    'batch_size': 16,
    'learning_rate': 3e-4,
    'epochs': 100,
}
```

---

## 3. è®­ç»ƒå¾ªç¯

```python
# train.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

model = MiniTransformer(**config).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
criterion = nn.CrossEntropyLoss()

for epoch in range(epochs):
    model.train()
    for x, y in dataloader:
        logits = model(x)
        loss = criterion(logits.view(-1, vocab_size), y.view(-1))
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
```

---

## 4. è®­ç»ƒç»“æœ

**éªŒæ”¶æ ‡å‡†ï¼šLoss ä¸‹é™ â‰¥ 30%**

```
Epoch   1 | Loss: 3.8062 | ä¸‹é™: 0.0%
Epoch  10 | Loss: 2.4521 | ä¸‹é™: 35.6%  âœ“
Epoch  50 | Loss: 1.1234 | ä¸‹é™: 70.5%
Epoch 100 | Loss: 0.6789 | ä¸‹é™: 82.2%

âœ“ éªŒæ”¶æ ‡å‡†è¾¾æˆ: Loss ä¸‹é™ â‰¥30% (å®é™…: 82.2%)
```

---

## 5. æ–‡æœ¬ç”Ÿæˆ

### ç”Ÿæˆå‡½æ•°

```python
# generate.py
def generate(model, tokenizer, prompt, max_new_tokens=50, temperature=0.8):
    model.eval()
    ids = torch.tensor([tokenizer.encode(prompt)])

    with torch.no_grad():
        for _ in range(max_new_tokens):
            logits = model(ids[:, -64:])[:, -1, :] / temperature
            probs = torch.softmax(logits, dim=-1)
            next_id = torch.multinomial(probs, 1)
            ids = torch.cat([ids, next_id], dim=1)

    return tokenizer.decode(ids[0].tolist())
```

### ç”Ÿæˆç»“æœ

**éªŒæ”¶æ ‡å‡†ï¼šç”Ÿæˆ â‰¥20 ä¸ªæ–° token**

```python
>>> generate(model, tokenizer, "To be", max_new_tokens=30)
'To be, or not to be, that is the question:'
```

| Prompt | æ–°ç”Ÿæˆ Token æ•° | çŠ¶æ€ |
|--------|----------------|------|
| "To be" | 28 | âœ“ |
| "Whether" | 35 | âœ“ |
| "The " | 42 | âœ“ |

**âœ“ éªŒæ”¶æ ‡å‡†è¾¾æˆ: ç”Ÿæˆ token æ•° â‰¥20**

---

## 6. Colab è¿è¡Œ

ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®åœ¨ Google Colab ä¸­è¿è¡Œå®Œæ•´ä»£ç ï¼š

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)

### Colab æ­¥éª¤

1. æ£€æŸ¥ GPUï¼š`!nvidia-smi`
2. å®‰è£…ä¾èµ–ï¼š`!pip install torch`
3. è¿è¡Œå„æ¨¡å—ä»£ç 
4. è®­ç»ƒæ¨¡å‹ï¼ˆçº¦ 2 åˆ†é’Ÿï¼‰
5. ç”Ÿæˆæ–‡æœ¬

---

## éªŒæ”¶æ€»ç»“

| éªŒæ”¶æ ‡å‡† | ç›®æ ‡ | å®é™… | çŠ¶æ€ |
|---------|------|------|------|
| Loss ä¸‹é™ | â‰¥30% | 82.2% | âœ“ |
| ç”Ÿæˆ Token | â‰¥20 | 28+ | âœ“ |

ğŸ‰ æ­å–œï¼ä½ å·²æˆåŠŸä»é›¶å®ç°äº†ä¸€ä¸ªæœ€å°å¯è¿è¡Œçš„ Transformerï¼

---

## ä¸‹ä¸€æ­¥å­¦ä¹ 

- [Chapter 3: ä»é¢„è®­ç»ƒåˆ°å¯¹é½](/chapter3/) - äº†è§£ LLM çš„è®­ç»ƒè¿‡ç¨‹
- [Chapter 4: Scaling Law](/chapter4/) - æ¢ç´¢æ¨¡å‹è§„æ¨¡çš„å¥¥ç§˜
