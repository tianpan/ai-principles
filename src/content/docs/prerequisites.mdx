---
title: 预备知识
description: 10 分钟快速回顾必要基础
---

# 预备知识

本章帮助你建立学习 Transformer 所需的最小知识基础。

---

## 1. 向量与矩阵

### 向量

**向量**是一组数字的有序列表，可以表示方向和大小。

```
v = [1, 2, 3, 4]  # 一个 4 维向量
```

**直觉理解**：向量就像一个"箭头"，有方向和长度。在 AI 中，我们用向量表示事物的"特征"。

### 矩阵

**矩阵**是数字的二维表格，可以表示变换。

```
M = [[1, 2],
     [3, 4],
     [5, 6]]    # 3×2 矩阵
```

**直觉理解**：矩阵可以"变换"向量——旋转、缩放、拉伸。神经网络就是用大量矩阵乘法来做计算的。

---

## 2. 点积（Dot Product）

两个向量的点积衡量它们的**"相似程度"**：

```
a · b = a₁×b₁ + a₂×b₂ + ... + aₙ×bₙ

例如：
[1, 2, 3] · [4, 5, 6] = 1×4 + 2×5 + 3×6 = 32
```

### 为什么点积重要？

点积越大 → 两个向量越"对齐" → **这就是 Attention 的核心直觉！**

```
查询向量 Q 和 键向量 K 的点积
  → 衡量"查询"与"键"的相似度
    → 决定"值向量 V"的权重
```

---

## 3. 神经网络直觉

神经网络就是一个巨大的**"函数拟合器"**：

```
输入数据 → [变换1] → [变换2] → ... → [变换N] → 输出结果
```

### 核心组件

| 组件 | 作用 |
|-----|-----|
| **权重** | 可学习的参数，决定如何变换 |
| **偏置** | 每层的偏移量 |
| **激活函数** | 引入非线性（如 ReLU、GELU） |

### 训练过程

1. **前向传播**：输入 → 经过网络 → 得到输出
2. **计算损失**：输出与目标的差距
3. **反向传播**：计算每个参数对损失的"贡献"
4. **更新参数**：用梯度下降调整参数，减小损失

---

## 4. Softmax

将任意数值转换为**概率分布**：

```
softmax(xᵢ) = e^xᵢ / Σⱼ e^xⱼ

例如：
输入: [2, 1, 0.1]
输出: [0.659, 0.242, 0.099]  # 和为 1
```

### 为什么需要 Softmax？

- 所有值都在 0-1 之间 → 可以理解为"概率"
- 所有值之和为 1 → 可以做"选择"决策
- **Attention 用 Softmax 把分数变成权重**

---

## 5. 快速自测

完成以下问题，检验你的理解：

### 问题 1
两个向量 `[1, 0]` 和 `[0, 1]` 的点积是多少？

:::details[答案]
0。这两个向量互相垂直（正交），点积为 0，表示"完全不相关"。
:::

### 问题 2
Softmax `[1, 1, 1]` 的结果是什么？

:::details[答案]
`[0.333, 0.333, 0.333]`。所有值相同，概率均等分布。
:::

### 问题 3
神经网络为什么要用激活函数？

:::details[答案]
引入非线性。如果没有激活函数，多层网络等价于单层线性变换，无法学习复杂模式。
:::

---

## 准备好了吗？

如果以上概念你都有基本了解，那就开始学习 Transformer 核心机制吧！

[开始学习 2.1 Token & Embedding →](/chapter2/1-token-embedding)

---

:::tip[学习建议]
如果某些概念还不够熟悉，不用担心。我们会在后续章节遇到具体概念时做进一步解释。重点是理解**直觉**，而不是死记公式。
:::
