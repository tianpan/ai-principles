{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mini Transformer Lab\n",
        "\n",
        "在 Google Colab 中运行最小 Transformer 模型\n",
        "\n",
        "**目标**:\n",
        "1. 理解 Transformer 各模块的代码实现\n",
        "2. 在 toy 数据集上完成训练\n",
        "3. 使用训练好的模型进行文本生成"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 检查 GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装依赖\n",
        "!pip install torch --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 实现各模块"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Token Embedding\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * (self.d_model ** 0.5)\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "print(\"✓ 模块定义完成\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        heads = torch.matmul(weights, V)\n",
        "        concat = heads.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "        return self.W_o(concat), weights\n",
        "\n",
        "print(\"✓ Multi-Head Attention 完成\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Block & Full Model\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.attention(self.norm1(x), mask)[0]\n",
        "        x = x + self.ffn(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2, d_ff=512, max_len=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.output = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.token_embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "        return self.output(self.norm(x))\n",
        "\n",
        "print(\"✓ 完整模型定义完成\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 训练"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 准备数据\n",
        "text = \"\"\"To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "\"\"\"\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "def encode(text): return [char_to_idx[c] for c in text]\n",
        "def decode(ids): return ''.join([idx_to_char[i] for i in ids])\n",
        "\n",
        "data = encode(text)\n",
        "print(f\"词表大小: {vocab_size}, Token 数量: {len(data)}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = MiniTransformer(vocab_size, d_model=64, num_heads=4, num_layers=2, d_ff=256).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "seq_len = 32\n",
        "X = torch.tensor([data[:seq_len]], dtype=torch.long).to(device)\n",
        "Y = torch.tensor([data[1:seq_len+1]], dtype=torch.long).to(device)\n",
        "\n",
        "model.train()\n",
        "initial_loss = None\n",
        "for epoch in range(200):\n",
        "    logits = model(X)\n",
        "    loss = criterion(logits.view(-1, vocab_size), Y.view(-1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if initial_loss is None: initial_loss = loss.item()\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "reduction = (initial_loss - loss.item()) / initial_loss * 100\n",
        "print(f\"\\nLoss 下降: {reduction:.1f}%\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 生成"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成文本\n",
        "def generate(model, prompt, max_new_tokens=50, temperature=0.8):\n",
        "    model.eval()\n",
        "    ids = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = model(ids[:, -64:])[:, -1, :] / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, 1)\n",
        "            ids = torch.cat([ids, next_id], dim=1)\n",
        "    return decode(ids[0].tolist())\n",
        "\n",
        "generated = generate(model, \"To be\", max_new_tokens=30)\n",
        "print(f\"生成结果:\\n{generated}\")\n",
        "print(f\"\\n生成 token 数: {len(generated) - len('To be')}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
