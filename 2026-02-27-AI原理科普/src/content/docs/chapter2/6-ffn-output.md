---
title: FFN ä¸è¾“å‡ºå±‚
description: ç†è§£å‰é¦ˆç½‘ç»œå’Œè¾“å‡ºå±‚
---

import TerminologyCard from '@components/common/TerminologyCard.astro';

# 2.6 FFN ä¸è¾“å‡ºå±‚

å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¸º Transformer æä¾›**éçº¿æ€§å˜æ¢**èƒ½åŠ›ã€‚

## Feed-Forward Network (FFN)

<TerminologyCard term="FFN" definition="ä¸¤ä¸ªçº¿æ€§å˜æ¢åŠ ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹ä½œç”¨ã€‚" />

### ç»“æ„

```
FFN(x) = GELU(xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚
```

é€šå¸¸ï¼š
- Wâ‚: d_model â†’ d_ffï¼ˆæ‰©å±•ï¼Œé€šå¸¸ d_ff = 4 Ã— d_modelï¼‰
- Wâ‚‚: d_ff â†’ d_modelï¼ˆå‹ç¼©å›åŸç»´åº¦ï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦ FFNï¼Ÿ

Self-Attention è´Ÿè´£**ä¿¡æ¯èšåˆ**ï¼ŒFFN è´Ÿè´£**ä¿¡æ¯å¤„ç†**ï¼š
- Attentionï¼šè®© token ä¹‹é—´äº¤æµ
- FFNï¼šå¯¹æ¯ä¸ª token çš„ä¿¡æ¯è¿›è¡ŒåŠ å·¥

ç±»æ¯”ï¼š
- Attention = è®¨è®ºä¼šè®®ï¼ˆå¤§å®¶äº¤æµä¿¡æ¯ï¼‰
- FFN = ç‹¬ç«‹æ€è€ƒï¼ˆæ¯ä¸ªäººæ•´ç†è‡ªå·±çš„ç¬”è®°ï¼‰

### æ¿€æ´»å‡½æ•°

| æ¿€æ´»å‡½æ•° | ä½¿ç”¨æ¨¡å‹ |
|----------|----------|
| ReLU | åŸå§‹ Transformer |
| GELU | BERT, GPT, LLaMA |
| Swish/GLU | PaLM, LLaMA 2 |

## è¾“å‡ºå±‚

ç»è¿‡å¤šå±‚ Transformer Block åï¼Œéœ€è¦å°† hidden state æ˜ å°„å›è¯è¡¨ã€‚

### Logits è®¡ç®—

```python
logits = hidden @ W_vocab  # [batch, seq_len, vocab_size]
```

### Softmax å¾—åˆ°æ¦‚ç‡

```python
probs = softmax(logits, dim=-1)  # [batch, seq_len, vocab_size]
```

### è¯­è¨€æ¨¡å‹å¤´

é€šå¸¸æœ‰ä¸¤ç§æ–¹å¼ï¼š
1. **å…±äº«æƒé‡**ï¼šW_vocab = Embedding^T
2. **ç‹¬ç«‹æƒé‡**ï¼šå•ç‹¬çš„çº¿æ€§å±‚

## å®Œæ•´çš„ Transformer Block

```python
class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        # Pre-LN Attention
        x = x + self.attention(self.norm1(x), mask)
        # Pre-LN FFN
        x = x + self.ffn(self.norm2(x))
        return x
```

## å®Œæ•´çš„ Transformer æ¨¡å‹

```python
class Transformer(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len=512):
        super().__init__()

        # Embedding + Positional Encoding
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)

        # Transformer Blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ])

        # æœ€ç»ˆ LayerNorm
        self.norm = nn.LayerNorm(d_model)

        # è¾“å‡ºå±‚ï¼ˆå…±äº«æƒé‡ï¼‰
        self.output = nn.Linear(d_model, vocab_size, bias=False)
        self.output.weight = self.token_embedding.weight

    def forward(self, x, mask=None):
        # Embedding
        x = self.token_embedding(x)
        x = self.pos_encoding(x)

        # Transformer Blocks
        for block in self.blocks:
            x = block(x, mask)

        # è¾“å‡º
        x = self.norm(x)
        logits = self.output(x)

        return logits
```

## å°ç»“

- **FFN**ï¼šæä¾›éçº¿æ€§å˜æ¢ï¼Œé€šå¸¸æ‰©å±• 4 å€å†å‹ç¼©
- **è¾“å‡ºå±‚**ï¼šå°† hidden state æ˜ å°„å›è¯è¡¨æ¦‚ç‡
- **å…±äº«æƒé‡**ï¼šå‡å°‘å‚æ•°é‡ï¼Œå¸¸ç”¨äºå°å‹æ¨¡å‹

---

ğŸ‰ **æ­å–œï¼** ä½ å·²ç»å­¦ä¹ äº† Transformer çš„æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ã€‚

[å‰å¾€ Mini Transformer Lab â†’](/lab/intro)
